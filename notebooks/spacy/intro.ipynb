{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up spaCy\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "# Test Data\n",
    "multiSentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
    "                 \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
    "                 \"In the beginning the Universe was created. This has made a lot of people \"\\\n",
    "                 \"very angry and been widely regarded as a bad move.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 640 There\n",
      "lowercased: 530 there\n",
      "lemma: 530 there\n",
      "shape: 489815 Xxxxx\n",
      "prefix: 2907 T\n",
      "suffix: 48458 ere\n",
      "log probability: -7.347356796264648\n",
      "Brown cluster id: 1918\n",
      "----------------------------------------\n",
      "original: 474 is\n",
      "lowercased: 474 is\n",
      "lemma: 488 be\n",
      "shape: 21581 xx\n",
      "prefix: 570 i\n",
      "suffix: 474 is\n",
      "log probability: -4.457748889923096\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 523 an\n",
      "lowercased: 523 an\n",
      "lemma: 523 an\n",
      "shape: 21581 xx\n",
      "prefix: 469 a\n",
      "suffix: 523 an\n",
      "log probability: -6.014852046966553\n",
      "Brown cluster id: 3\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# all you have to do to parse text is this:\n",
    "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "parsedData = parser(multiSentence)\n",
    "\n",
    "# Let's look at the tokens\n",
    "# All you have to do is iterate through the parsedData\n",
    "# Each token is an object with lots of different properties\n",
    "# A property with an underscore at the end returns the string representation\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# The probability estimate is based on counts from a 3 billion word\n",
    "# corpus, smoothed using the Simple Good-Turing method.\n",
    "for i, token in enumerate(parsedData):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an art, it says, or rather, a knack to flying.\n",
      "The knack lies in learning how to throw yourself at the ground and miss.\n",
      "In the beginning the Universe was created.\n",
      "This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the sentences\n",
    "sents = []\n",
    "# the \"sents\" property returns spans\n",
    "# spans have indices into the original string\n",
    "# where each index value represents a token\n",
    "for span in parsedData.sents:\n",
    "    # go from the start to the end of each span, returning each token in the sentence\n",
    "    # combine each token using join()\n",
    "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "\n",
    "for sentence in sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There ADV\n",
      "is VERB\n",
      "an DET\n",
      "art NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "says VERB\n",
      ", PUNCT\n",
      "or CONJ\n",
      "rather ADV\n",
      ", PUNCT\n",
      "a DET\n",
      "knack NOUN\n",
      "to ADP\n",
      "flying NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the part of speech tags of the first sentence\n",
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "for token in sent:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det boy [] []\n",
      "boy nsubj ran ['The'] ['with']\n",
      "with prep boy [] ['dog']\n",
      "the det dog [] []\n",
      "spotted amod dog [] []\n",
      "dog pobj with ['the', 'spotted'] []\n",
      "quickly advmod ran [] []\n",
      "ran ROOT ran ['boy', 'quickly'] ['after', '.']\n",
      "after prep ran [] ['firetruck']\n",
      "the det firetruck [] []\n",
      "firetruck pobj after ['the'] []\n",
      ". punct ran [] []\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the dependencies of this example:\n",
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsedEx = parser(example)\n",
    "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nearby (not an entity)\n",
      "pubs (not an entity)\n",
      "-------------- entities only ---------------\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the named entities of this example:\n",
    "example = \"nearby pubs\"\n",
    "parsedEx = parser(example)\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n",
    "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
    "ents = list(parsedEx.ents)\n",
    "for entity in ents:\n",
    "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar words to NASA:\n",
      "bars\n",
      "restaurant\n",
      "nightclub\n",
      "bistro\n",
      "laundromat\n",
      "launderette\n",
      "brewpub\n",
      "barroom\n",
      "cantina\n",
      "postbox\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# you can access known words from the parser's vocabulary\n",
    "nasa = parser.vocab['bar']\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list([w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"bar\"])\n",
    "\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, nasa.repvec))\n",
    "allWords.reverse()\n",
    "print(\"Top 10 most similar words to NASA:\")\n",
    "for word in allWords[:10]:   \n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------\n",
      "Top 3 closest results for king - man + woman:\n",
      "bed-and-breakfast\n",
      "kitchenette\n",
      "scullery\n"
     ]
    }
   ],
   "source": [
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab['nearby']\n",
    "man = parser.vocab['bar']\n",
    "woman = parser.vocab['food']\n",
    "\n",
    "result = king.repvec + man.repvec + woman.repvec\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list([w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"nearby\" and w.lower_ != \"bar\" and w.lower_ != \"food\"])\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.repvec, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from subject_object_extraction import findSVOs\n",
    "\n",
    "# can still work even without punctuation\n",
    "parse = parser(\"best food nearby\")\n",
    "print(findSVOs(parse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
